#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å•é¡Œé‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
è¿½åŠ ã•ã‚ŒãŸå•é¡Œã¨æ—¢å­˜å•é¡Œã®æ„å‘³çš„é‡è¤‡ã‚’æ¤œè¨¼
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Set, Tuple
from difflib import SequenceMatcher

def load_json_file(file_path: str) -> List[Dict]:
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å•é¡Œã‚’èª­ã¿è¾¼ã¿"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('questions', [])
    except FileNotFoundError:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        return []

def normalize_text(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–ï¼ˆæ¯”è¼ƒç”¨ï¼‰"""
    # å…¨è§’ãƒ»åŠè§’çµ±ä¸€ã€è¨˜å·é™¤å»ã€ç©ºç™½é™¤å»
    text = re.sub(r'[ï¼ˆï¼‰()ã€ã€‘ã€Œã€ã€ã€\s]', '', text)
    text = text.replace('ï¼Ÿ', '').replace('?', '')
    text = text.replace('ã€‚', '').replace('ã€', '')
    return text.lower()

def extract_key_terms(question: str) -> Set[str]:
    """å•é¡Œæ–‡ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    # é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
    patterns = [
        r'[å¹´ä»£]\d+å¹´',  # å¹´ä»£
        r'[å¹³éŒå®¤æ±Ÿæ˜å¤§æ˜­]\w+æ™‚ä»£',  # æ™‚ä»£å
        r'\w+[å¯ºé™¢ç¥ç¤¾]',  # å¯ºç¤¾å
        r'\w+[å¤©çš‡å°†è»]',  # äººç‰©
        r'\w+[ç¥­ã‚Šç¥­]',  # ç¥­ã‚Š
        r'\w+[é€šã‚Šé“]',  # åœ°å
        r'\w+[ç¹”ç„¼]',  # å·¥èŠ¸å“
    ]
    
    terms = set()
    text = question
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
    for pattern in patterns:
        matches = re.findall(pattern, text)
        terms.update(matches)
    
    # é‡è¦ãªå›ºæœ‰åè©ã‚’æŠ½å‡ºï¼ˆã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã®é€£ç¶šï¼‰
    proper_nouns = re.findall(r'[ã‚¢-ãƒ²]{2,}|[ä¸€-é¾¯]{2,}', text)
    terms.update([noun for noun in proper_nouns if len(noun) >= 2])
    
    return terms

def calculate_similarity(q1: Dict, q2: Dict) -> Tuple[float, str]:
    """2ã¤ã®å•é¡Œã®é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
    
    # å•é¡Œæ–‡ã®é¡ä¼¼åº¦
    text1 = normalize_text(q1['question'])
    text2 = normalize_text(q2['question'])
    text_similarity = SequenceMatcher(None, text1, text2).ratio()
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é‡è¤‡åº¦
    terms1 = extract_key_terms(q1['question'])
    terms2 = extract_key_terms(q2['question'])
    
    if len(terms1) == 0 and len(terms2) == 0:
        keyword_similarity = 0.0
    else:
        common_terms = terms1 & terms2
        total_terms = terms1 | terms2
        keyword_similarity = len(common_terms) / len(total_terms) if total_terms else 0.0
    
    # é¸æŠè‚¢ã®é¡ä¼¼åº¦
    options1 = set(normalize_text(opt) for opt in q1['options'])
    options2 = set(normalize_text(opt) for opt in q2['options'])
    
    if len(options1) == 0 and len(options2) == 0:
        option_similarity = 0.0
    else:
        common_options = options1 & options2
        total_options = options1 | options2
        option_similarity = len(common_options) / len(total_options) if total_options else 0.0
    
    # ç·åˆé¡ä¼¼åº¦ï¼ˆé‡ã¿ä»˜ã‘å¹³å‡ï¼‰
    total_similarity = (
        text_similarity * 0.5 +
        keyword_similarity * 0.3 +
        option_similarity * 0.2
    )
    
    # é¡ä¼¼ã®ç†ç”±
    reasons = []
    if text_similarity > 0.6:
        reasons.append(f"å•é¡Œæ–‡é¡ä¼¼åº¦: {text_similarity:.2f}")
    if keyword_similarity > 0.5:
        reasons.append(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é‡è¤‡: {keyword_similarity:.2f}")
        if len(terms1 & terms2) > 0:
            reasons.append(f"å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(list(terms1 & terms2)[:3])}")
    if option_similarity > 0.5:
        reasons.append(f"é¸æŠè‚¢é‡è¤‡: {option_similarity:.2f}")
    
    reason = " | ".join(reasons) if reasons else "é¡ä¼¼åº¦ä½"
    
    return total_similarity, reason

def check_duplicates_comprehensive():
    """åŒ…æ‹¬çš„ãªé‡è¤‡ãƒã‚§ãƒƒã‚¯"""
    
    # æ—¢å­˜ã®å•é¡Œã‚’èª­ã¿è¾¼ã¿ï¼ˆæ–°è¦è¿½åŠ å‰ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ï¼‰
    # ã¾ãšç¾åœ¨ã®questions.jsonã‹ã‚‰æ–°è¦è¿½åŠ åˆ†ã‚’é™¤ã„ãŸå…ƒãƒ‡ãƒ¼ã‚¿ã‚’æ¨å®š
    current_questions = load_json_file("public/data/questions.json")
    new_3kyuu = load_json_file("additional_3kyuu_questions.json")
    new_2kyuu = load_json_file("additional_2kyuu_questions.json")
    
    print("=== é‡è¤‡ãƒã‚§ãƒƒã‚¯é–‹å§‹ ===")
    print(f"ç¾åœ¨ã®ç·å•é¡Œæ•°: {len(current_questions)}")
    print(f"æ–°è¦3ç´šå•é¡Œ: {len(new_3kyuu)}")
    print(f"æ–°è¦2ç´šå•é¡Œ: {len(new_2kyuu)}")
    
    # æ—¢å­˜å•é¡Œã‚’æ¨å®šï¼ˆæ–°è¦è¿½åŠ åˆ†ã‚’é™¤ãï¼‰
    # IDã®ç¯„å›²ã‹ã‚‰æ¨å®š
    max_existing_id = 487  # ä½œæ¥­å‰ã®å•é¡Œæ•°
    existing_questions = [q for q in current_questions 
                         if q['id'].startswith('q') and 
                         int(q['id'][1:]) <= max_existing_id]
    
    print(f"æ—¢å­˜å•é¡Œæ•°ï¼ˆæ¨å®šï¼‰: {len(existing_questions)}")
    
    # æ–°è¦è¿½åŠ å•é¡Œã‚’ã¾ã¨ã‚ã‚‹
    all_new_questions = new_3kyuu + new_2kyuu
    
    duplicates_found = []
    high_similarity_pairs = []
    
    print("\n=== æ–°è¦å•é¡Œ vs æ—¢å­˜å•é¡Œã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ ===")
    
    for new_q in all_new_questions:
        print(f"\nğŸ“ æ–°è¦å•é¡Œ: {new_q['id']} ({new_q['level']}, {new_q['category']})")
        print(f"   å•é¡Œ: {new_q['question'][:60]}...")
        
        best_match = None
        best_similarity = 0.0
        
        for existing_q in existing_questions:
            # åŒã˜ç´šã¨ã‚«ãƒ†ã‚´ãƒªã®å•é¡Œã®ã¿ãƒã‚§ãƒƒã‚¯
            if (new_q['level'] == existing_q['level'] and 
                new_q['category'] == existing_q['category']):
                
                similarity, reason = calculate_similarity(new_q, existing_q)
                
                if similarity > 0.4:  # é–¾å€¤: 40%ä»¥ä¸Šã§è¦æ³¨æ„
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_match = existing_q
        
        if best_match:
            if best_similarity > 0.7:  # 70%ä»¥ä¸Šã§é‡è¤‡ã¨ã¿ãªã™
                duplicates_found.append({
                    'new_question': new_q,
                    'existing_question': best_match,
                    'similarity': best_similarity,
                    'reason': reason
                })
                print(f"   âš ï¸ é‡è¤‡ç–‘ã„: {best_similarity:.2f} - {best_match['question'][:40]}...")
            elif best_similarity > 0.4:  # 40%ä»¥ä¸Šã§é¡ä¼¼ã¨ã—ã¦å ±å‘Š
                high_similarity_pairs.append({
                    'new_question': new_q,
                    'existing_question': best_match,
                    'similarity': best_similarity,
                    'reason': reason
                })
                print(f"   âš¡ é¡ä¼¼æ³¨æ„: {best_similarity:.2f} - {best_match['question'][:40]}...")
            else:
                print(f"   âœ… å•é¡Œãªã—: {best_similarity:.2f}")
        else:
            print(f"   âœ… åŒã‚«ãƒ†ã‚´ãƒªå†…ã§é‡è¤‡ãªã—")
    
    print("\n=== æ–°è¦å•é¡Œé–“ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ ===")
    
    new_vs_new_duplicates = []
    
    for i, q1 in enumerate(all_new_questions):
        for j, q2 in enumerate(all_new_questions[i+1:], i+1):
            if (q1['level'] == q2['level'] and 
                q1['category'] == q2['category']):
                
                similarity, reason = calculate_similarity(q1, q2)
                
                if similarity > 0.6:  # æ–°è¦å•é¡Œé–“ã¯60%ã§è¦æ³¨æ„
                    new_vs_new_duplicates.append({
                        'question1': q1,
                        'question2': q2,
                        'similarity': similarity,
                        'reason': reason
                    })
    
    # çµæœå ±å‘Š
    print("\n" + "="*60)
    print("ğŸ“Š é‡è¤‡ãƒã‚§ãƒƒã‚¯çµæœ")
    print("="*60)
    
    if duplicates_found:
        print(f"\nğŸš¨ é‡è¤‡å•é¡Œç™ºè¦‹: {len(duplicates_found)}ä»¶")
        for dup in duplicates_found:
            print(f"\né‡è¤‡åº¦: {dup['similarity']:.2f}")
            print(f"æ–°è¦: {dup['new_question']['question']}")
            print(f"æ—¢å­˜: {dup['existing_question']['question']}")
            print(f"ç†ç”±: {dup['reason']}")
    else:
        print(f"\nâœ… é‡è¤‡å•é¡Œ: 0ä»¶")
    
    if high_similarity_pairs:
        print(f"\nâš ï¸ é¡ä¼¼å•é¡Œï¼ˆè¦ç¢ºèªï¼‰: {len(high_similarity_pairs)}ä»¶")
        for pair in high_similarity_pairs:
            print(f"\né¡ä¼¼åº¦: {pair['similarity']:.2f}")
            print(f"æ–°è¦: {pair['new_question']['question']}")
            print(f"æ—¢å­˜: {pair['existing_question']['question']}")
            print(f"ç†ç”±: {pair['reason']}")
    else:
        print(f"\nâœ… é¡ä¼¼å•é¡Œï¼ˆè¦ç¢ºèªï¼‰: 0ä»¶")
    
    if new_vs_new_duplicates:
        print(f"\nğŸ”„ æ–°è¦å•é¡Œé–“é‡è¤‡: {len(new_vs_new_duplicates)}ä»¶")
        for dup in new_vs_new_duplicates:
            print(f"\né¡ä¼¼åº¦: {dup['similarity']:.2f}")
            print(f"å•é¡Œ1: {dup['question1']['question']}")
            print(f"å•é¡Œ2: {dup['question2']['question']}")
    else:
        print(f"\nâœ… æ–°è¦å•é¡Œé–“é‡è¤‡: 0ä»¶")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥æ–°è¦å•é¡Œçµ±è¨ˆ
    print(f"\nğŸ“ˆ ã‚«ãƒ†ã‚´ãƒªåˆ¥æ–°è¦å•é¡Œçµ±è¨ˆ")
    category_stats = {}
    for q in all_new_questions:
        key = f"{q['level']}_{q['category']}"
        category_stats[key] = category_stats.get(key, 0) + 1
    
    for key, count in sorted(category_stats.items()):
        level, category = key.split('_', 1)
        print(f"  {level} {category}: {count}å•")
    
    return duplicates_found, high_similarity_pairs, new_vs_new_duplicates

if __name__ == "__main__":
    check_duplicates_comprehensive()